{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "579586e7-b135-4d30-979e-99c38fc3399d",
   "metadata": {},
   "source": [
    "# Mujoco + torchrl Demo using PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9dedb5-0bfc-44c4-b66e-9eea3c653f7b",
   "metadata": {},
   "source": [
    "Main reference: https://pytorch.org/rl/stable/tutorials/coding_ppo.html\n",
    "\n",
    "This notebook demo training neural network to solve the `InvertedDoublePendulum-v4` enviornment in legacy OpenAI `gym`. The tech stack is `mujoco` (required by this reinforcement learning enviornment/task) and `torchrl` for the RL framework/library. Algorithm used is **PPO (Proximal Policy Optimization)**.\n",
    "\n",
    "We simply copy the notebook from the main reference but strip almost all text for streamlined execution, please read the reference first for the exposition.\n",
    "\n",
    "We then extend it to add some basic features:\n",
    "- Save/Load the model\n",
    "- Run the environment using our neural network, with ability to save the video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83c5502-cbd4-4b2e-9237-65473d53b34f",
   "metadata": {},
   "source": [
    "Assume you've already installed all system level and python dependencies.\n",
    "\n",
    "First, let's check the env. You should have the `MUJOCO_GL=osmesa` and `PYOPENGL_PLATFORM=osmesa` environment variables set below. If not, add it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6b1bab-8aca-4868-ba60-5bb9507ff2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc33bc9-6cab-464d-9737-1eecf8183427",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5568091d-4b54-4206-96c2-af11c1b596e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from torch import multiprocessing\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tensordict.nn import TensorDictModule\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "from torch import nn\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data.replay_buffers import ReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "from torchrl.envs import (Compose, DoubleToFloat, ObservationNorm, StepCounter,\n",
    "                          TransformedEnv)\n",
    "from torchrl.envs.libs.gym import GymEnv\n",
    "from torchrl.envs.utils import check_env_specs, ExplorationType, set_exploration_type\n",
    "from torchrl.modules import ProbabilisticActor, TanhNormal, ValueOperator\n",
    "from torchrl.objectives import ClipPPOLoss\n",
    "from torchrl.objectives.value import GAE\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4324a5a6-5c8d-4974-a3b1-6909a303836f",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_fork = multiprocessing.get_start_method() == \"fork\"\n",
    "device = (\n",
    "    torch.device(0)\n",
    "    if torch.cuda.is_available() and not is_fork\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "num_cells = 256  # number of cells in each layer i.e. output dim.\n",
    "lr = 3e-4\n",
    "max_grad_norm = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4471c18c-6ab8-4e4d-b009-da3d4deb396e",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_per_batch = 1000\n",
    "# For a complete training, bring the number of frames up to 1M\n",
    "total_frames = 50_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6daf5c-8789-4231-80c1-dd7bd5d1279e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_batch_size = 64  # cardinality of the sub-samples gathered from the current data in the inner loop\n",
    "num_epochs = 10  # optimization steps per batch of data collected\n",
    "clip_epsilon = (\n",
    "    0.2  # clip value for PPO loss: see the equation in the intro for more context.\n",
    ")\n",
    "gamma = 0.99\n",
    "lmbda = 0.95\n",
    "entropy_eps = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a630d9da-8c72-491a-85a3-7714dff33163",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_env = GymEnv(\"InvertedDoublePendulum-v4\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc47b01f-989b-4eed-ad64-1eb23db971ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TransformedEnv(\n",
    "    base_env,\n",
    "    Compose(\n",
    "        # normalize observations\n",
    "        ObservationNorm(in_keys=[\"observation\"]),\n",
    "        DoubleToFloat(),\n",
    "        StepCounter(),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f41952-4243-4216-8a95-17ee7e146565",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.transform[0].init_stats(num_iter=1000, reduce_dim=0, cat_dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5f41df-9e69-4cc1-8991-b4b9effd4ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"normalization constant shape:\", env.transform[0].loc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c49f6f-93a7-494a-9bb3-ba8ce69d392d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"observation_spec:\", env.observation_spec)\n",
    "print(\"reward_spec:\", env.reward_spec)\n",
    "print(\"input_spec:\", env.input_spec)\n",
    "print(\"action_spec (as defined by input_spec):\", env.action_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb610d7-8c76-49b1-82ae-77c1fdf33eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_env_specs(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f39712-60f5-4e84-b4a1-04cf8de1d10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout = env.rollout(3)\n",
    "print(\"rollout of three steps:\", rollout)\n",
    "print(\"Shape of the rollout TensorDict:\", rollout.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df656c8a-15a8-419b-9b44-d419dd4aa64a",
   "metadata": {},
   "source": [
    "## Define Neural Networks etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07380515-38d7-4992-b25d-c31cc926dd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_net = nn.Sequential(\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(2 * env.action_spec.shape[-1], device=device),\n",
    "    NormalParamExtractor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7b6948-1484-4e7b-8256-2075b6a1c27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_module = TensorDictModule(\n",
    "    actor_net, in_keys=[\"observation\"], out_keys=[\"loc\", \"scale\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5f2d42-5c7f-479c-aba7-8301d4f7474b",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_module = ProbabilisticActor(\n",
    "    module=policy_module,\n",
    "    spec=env.action_spec,\n",
    "    in_keys=[\"loc\", \"scale\"],\n",
    "    distribution_class=TanhNormal,\n",
    "    distribution_kwargs={\n",
    "        \"low\": env.action_spec.space.low,\n",
    "        \"high\": env.action_spec.space.high,\n",
    "    },\n",
    "    return_log_prob=True,\n",
    "    # we'll need the log-prob for the numerator of the importance weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68fde16-2917-4013-ac0a-63fdbcf19931",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_net = nn.Sequential(\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(1, device=device),\n",
    ")\n",
    "\n",
    "value_module = ValueOperator(\n",
    "    module=value_net,\n",
    "    in_keys=[\"observation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167f1017-b69e-4b9f-a601-0eb0b106c4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running policy:\", policy_module(env.reset()))\n",
    "print(\"Running value:\", value_module(env.reset()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ba9c27-8d66-4e1b-abfa-af487cf81e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "advantage_module = GAE(\n",
    "    gamma=gamma, lmbda=lmbda, value_network=value_module, average_gae=True\n",
    ")\n",
    "\n",
    "loss_module = ClipPPOLoss(\n",
    "    actor_network=policy_module,\n",
    "    critic_network=value_module,\n",
    "    clip_epsilon=clip_epsilon,\n",
    "    entropy_bonus=bool(entropy_eps),\n",
    "    entropy_coef=entropy_eps,\n",
    "    # these keys match by default but we set this for completeness\n",
    "    critic_coef=1.0,\n",
    "    loss_critic_type=\"smooth_l1\",\n",
    ")\n",
    "\n",
    "optim = torch.optim.Adam(loss_module.parameters(), lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optim, total_frames // frames_per_batch, 0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c819e35-8057-404e-9681-2cd8ae7f8b0f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### RL-specific setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42b275b-4814-4a82-bcfa-d0845497b47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = SyncDataCollector(\n",
    "    env,\n",
    "    policy_module,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    total_frames=total_frames,\n",
    "    split_trajs=False,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e06cc7-d58e-4edd-bf52-69085a8382cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(\n",
    "    storage=LazyTensorStorage(max_size=frames_per_batch),\n",
    "    sampler=SamplerWithoutReplacement(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7033ca1c-5396-463d-a5c7-2d806332551c",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfe74c4-9dcf-4ebf-99e6-2f629cad2b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = defaultdict(list)\n",
    "pbar = tqdm(total=total_frames)\n",
    "eval_str = \"\"\n",
    "\n",
    "# We iterate over the collector until it reaches the total number of frames it was\n",
    "# designed to collect:\n",
    "for i, tensordict_data in enumerate(collector):\n",
    "    # we now have a batch of data to work with. Let's learn something from it.\n",
    "    for _ in range(num_epochs):\n",
    "        # We'll need an \"advantage\" signal to make PPO work.\n",
    "        # We re-compute it at each epoch as its value depends on the value\n",
    "        # network which is updated in the inner loop.\n",
    "        advantage_module(tensordict_data)\n",
    "        data_view = tensordict_data.reshape(-1)\n",
    "        replay_buffer.extend(data_view.cpu())\n",
    "        for _ in range(frames_per_batch // sub_batch_size):\n",
    "            subdata = replay_buffer.sample(sub_batch_size)\n",
    "            loss_vals = loss_module(subdata.to(device))\n",
    "            loss_value = (\n",
    "                loss_vals[\"loss_objective\"]\n",
    "                + loss_vals[\"loss_critic\"]\n",
    "                + loss_vals[\"loss_entropy\"]\n",
    "            )\n",
    "\n",
    "            # Optimization: backward, grad clipping and optimization step\n",
    "            loss_value.backward()\n",
    "            # this is not strictly mandatory but it's good practice to keep\n",
    "            # your gradient norm bounded\n",
    "            torch.nn.utils.clip_grad_norm_(loss_module.parameters(), max_grad_norm)\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "    logs[\"reward\"].append(tensordict_data[\"next\", \"reward\"].mean().item())\n",
    "    pbar.update(tensordict_data.numel())\n",
    "    cum_reward_str = (\n",
    "        f\"average reward={logs['reward'][-1]: 4.4f} (init={logs['reward'][0]: 4.4f})\"\n",
    "    )\n",
    "    logs[\"step_count\"].append(tensordict_data[\"step_count\"].max().item())\n",
    "    stepcount_str = f\"step count (max): {logs['step_count'][-1]}\"\n",
    "    logs[\"lr\"].append(optim.param_groups[0][\"lr\"])\n",
    "    lr_str = f\"lr policy: {logs['lr'][-1]: 4.4f}\"\n",
    "    if i % 10 == 0:\n",
    "        # We evaluate the policy once every 10 batches of data.\n",
    "        # Evaluation is rather simple: execute the policy without exploration\n",
    "        # (take the expected value of the action distribution) for a given\n",
    "        # number of steps (1000, which is our ``env`` horizon).\n",
    "        # The ``rollout`` method of the ``env`` can take a policy as argument:\n",
    "        # it will then execute this policy at each step.\n",
    "        with set_exploration_type(ExplorationType.DETERMINISTIC), torch.no_grad():\n",
    "            # execute a rollout with the trained policy\n",
    "            eval_rollout = env.rollout(1000, policy_module)\n",
    "            logs[\"eval reward\"].append(eval_rollout[\"next\", \"reward\"].mean().item())\n",
    "            logs[\"eval reward (sum)\"].append(\n",
    "                eval_rollout[\"next\", \"reward\"].sum().item()\n",
    "            )\n",
    "            logs[\"eval step_count\"].append(eval_rollout[\"step_count\"].max().item())\n",
    "            eval_str = (\n",
    "                f\"eval cumulative reward: {logs['eval reward (sum)'][-1]: 4.4f} \"\n",
    "                f\"(init: {logs['eval reward (sum)'][0]: 4.4f}), \"\n",
    "                f\"eval step-count: {logs['eval step_count'][-1]}\"\n",
    "            )\n",
    "            del eval_rollout\n",
    "    pbar.set_description(\", \".join([eval_str, cum_reward_str, stepcount_str, lr_str]))\n",
    "\n",
    "    # We're also using a learning rate scheduler. Like the gradient clipping,\n",
    "    # this is a nice-to-have but nothing necessary for PPO to work.\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00d0eae-6d06-4b3c-b7c8-5ebcdce6f085",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(logs[\"reward\"])\n",
    "plt.title(\"training rewards (average)\")\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(logs[\"step_count\"])\n",
    "plt.title(\"Max step count (training)\")\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(logs[\"eval reward (sum)\"])\n",
    "plt.title(\"Return (test)\")\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(logs[\"eval step_count\"])\n",
    "plt.title(\"Max step count (test)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f0f998-aaa7-4b60-bee6-1347cd3f31db",
   "metadata": {},
   "source": [
    "## Save/Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d066ba-3a6b-4200-9c5d-8e244610d976",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(actor_net.state_dict(), \"actor.pt\")\n",
    "torch.save(value_net.state_dict(), \"value.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8384e187-c85d-478c-bd81-ed9b2e4fe2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#actor_net.load_state_dict(torch.load(\"actor.pt\", weights_only=True))\n",
    "# TODO similar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a34f671-42c6-48a4-9ba9-a0cea4d1a427",
   "metadata": {},
   "source": [
    "## Running the Env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3e0f70-9e53-42e9-bd6e-00064102fc4c",
   "metadata": {},
   "source": [
    "### Direct run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6c5b0e-5cda-4d91-8488-bde35f25b8a1",
   "metadata": {},
   "source": [
    "Reference:\n",
    "- https://pytorch.org/rl/stable/tutorials/torchrl_demo.html#using-environments-and-modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76243d3b-70b4-4aea-a817-3d897c50271b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.envs.utils import step_mdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926b9e48-4c84-4c0b-a49b-fd39a65b37b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c1495b-f78c-4a66-be21-46d05b7632d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e72e4d7-ed37-4206-b4a0-ced81bb7e2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# equivalent\n",
    "torch.manual_seed(1337)\n",
    "env.set_seed(42)\n",
    "\n",
    "max_steps = 100\n",
    "tensordict = env.reset()\n",
    "tensordicts = []\n",
    "for _ in range(max_steps):\n",
    "    policy_module(tensordict)\n",
    "    tensordicts.append(env.step(tensordict))\n",
    "    if tensordict[\"done\"].any():\n",
    "        break\n",
    "    tensordict = step_mdp(tensordict)  # roughly equivalent to obs = next_obs\n",
    "tensordicts_stack = torch.stack(tensordicts, 0)\n",
    "#print(\"total steps:\", i)\n",
    "print(tensordicts_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9921800-8c93-4e59-b013-6b9bb8d13c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "env.set_seed(42)\n",
    "tensordict_rollout = env.rollout(policy=policy_module, max_steps=max_steps)\n",
    "tensordict_rollout\n",
    "\n",
    "\n",
    "#(tensordict_rollout == tensordicts_stack).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1eafd8-897c-4e27-927a-c449341c717d",
   "metadata": {},
   "source": [
    "### Run and save video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcfbd5b-48a1-49a1-98ab-772cade5232a",
   "metadata": {},
   "source": [
    "References:\n",
    "- https://github.com/pytorch/rl/issues/1400\n",
    "- https://pytorch.org/rl/stable/tutorials/getting-started-5.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0ba541-d8d2-4bc5-b34f-039d6cb8da24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchrl.envs import ParallelEnv\n",
    "from torchrl.record import VideoRecorder, CSVLogger\n",
    "#from torchrl.record.loggers.wandb import WandbLogger\n",
    "\n",
    "\n",
    "def make_env(path):\n",
    "    #logger = WandbLogger(exp_name=\"ppo\", project=\"test\")\n",
    "    logger = CSVLogger(exp_name=\"demo\", log_dir=path, video_format=\"mp4\")\n",
    "\n",
    "    inner_env = TransformedEnv(\n",
    "        GymEnv(\n",
    "            \"InvertedDoublePendulum-v4\", \n",
    "            from_pixels=True,\n",
    "            pixels_only=False,\n",
    "            device=\"cpu\", \n",
    "            # frame_skip=frame_skip, \n",
    "            render_mode=\"rgb_array\"\n",
    "        ),\n",
    "        Compose(\n",
    "            VideoRecorder(logger=logger, tag=\"ppo video\"),\n",
    "            # normalize observations\n",
    "            ObservationNorm(in_keys=[\"observation\"]),\n",
    "            DoubleToFloat(in_keys=[\"observation\"]),\n",
    "            StepCounter(),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    inner_env.transform[1].init_stats(num_iter=1000, reduce_dim=0, cat_dim=0)\n",
    "\n",
    "    return inner_env\n",
    "\n",
    "record_env = make_env(\"./trainging_loop\")\n",
    "\n",
    "check_env_specs(record_env)\n",
    "\n",
    "# env = ParallelEnv(3, make_env)\n",
    "\n",
    "print(\"env:\", record_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30df9466-afa1-490d-bfc2-b0e36ffdc4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_rollout = record_env.rollout(100, policy_module)\n",
    "record_env.transform[0].dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548f2ef4-4d88-48bc-a82e-bb70f7220e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
